{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Final_Models.ipynb","provenance":[{"file_id":"1XFFdliWshaD-2loZBNQEc0CNjZaClsWJ","timestamp":1575349379950}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"YJwe1oq0yUQa","outputId":"1cc95256-805c-4290-dfca-297d501e5e87","executionInfo":{"status":"ok","timestamp":1575552484657,"user_tz":300,"elapsed":4042,"user":{"displayName":"Ravi Choudhary","photoUrl":"","userId":"10457362382565872664"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["import pandas as pd\n","from pandas import DataFrame\n","df = pd.read_csv('a391d853147b-NASA_DataSets_Scrub.tsv', sep='\\t')\n","import copy\n","import torch\n","import torch.nn as nn\n","from torch.nn import Parameter\n","import torch.nn.functional as F\n","import math\n","import numpy as np\n","from torch.autograd import Variable, Function\n","import pickle\n","import os\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from gensim.models.coherencemodel import CoherenceModel\n","import gensim\n","from gensim.utils import simple_preprocess\n","from gensim.parsing.preprocessing import STOPWORDS\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk.stem.porter import *\n","import numpy as np\n","np.random.seed(2018)\n","import nltk\n","n_top_words = 20\n","nltk.download('wordnet')\n","\n","if torch.cuda.device_count() == 0:\n","    device = 'cpu'\n","else:\n","    device = 'cuda'\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zwYMjRE_G4q6","colab":{}},"source":["stemmer = SnowballStemmer('english')\n","# def lemmatize_stemming(text):\n","#     return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n","\n","def lemmatize(text):\n","    return WordNetLemmatizer().lemmatize(text, pos='v')\n","\n","\n","def preprocess(text):\n","    result = []\n","    for token in gensim.utils.simple_preprocess(text):\n","        if token not in gensim.parsing.preprocessing.STOPWORDS and len(set(token))>1:\n","            result.append(lemmatize(token))\n","    return result"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5mXnk2d7HfxH","colab":{}},"source":["df = df.dropna()\n","df['desc_title'] = df['description'] + '\\n' + df['title']\n","df['tokenized_desc'] = df['description'].map(preprocess)\n","df['tokenized_title'] = df['title'].map(preprocess)\n","df['tokenized_desc_title'] = df['tokenized_desc'] + df['tokenized_title']\n","df = df[df['tokenized_desc_title'].str.len()!=0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Z8TyHeAnKMXw","colab":{}},"source":["dictionary = gensim.corpora.Dictionary(df['tokenized_desc_title'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iE4Xd1n4KRxA","colab":{}},"source":["bow_corpus = [dictionary.doc2bow(doc) for doc in df['tokenized_desc_title']]\n","\n","from gensim import corpora, models\n","\n","tfidf = models.TfidfModel(bow_corpus)\n","corpus_tfidf = tfidf[bow_corpus]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"q21qXeBXL8Je","colab":{}},"source":["def compute_coherence_LDA(dictionary, corpus, texts, limit, start=2, step=3):\n","    \"\"\"\n","    Compute c_v coherence for various number of topics\n","\n","    Parameters:\n","    ----------\n","    dictionary : Gensim dictionary\n","    corpus : Gensim corpus\n","    texts : List of input texts\n","    limit : Max num of topics\n","\n","    Returns:\n","    -------\n","    model_list : List of LDA topic models\n","    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n","    \"\"\"\n","    coherence_values = []\n","    model_list = []\n","    for num_topics in range(start, limit, step):\n","        model=gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=2, workers=2, dtype=np.float64)\n","        model_list.append(model)\n","        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v', topn = n_top_words)\n","        coherence_values.append(coherencemodel.get_coherence())\n","\n","    return model_list, coherence_values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gbTj3CZDWqOS","outputId":"9428044c-5d78-4feb-90f2-779cba185cdb","executionInfo":{"status":"ok","timestamp":1575552568933,"user_tz":300,"elapsed":84284,"user":{"displayName":"Ravi Choudhary","photoUrl":"","userId":"10457362382565872664"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["model_list, coherence_values = compute_coherence_LDA(dictionary=dictionary, corpus=corpus_tfidf, texts=df['tokenized_desc_title'], start=5, limit=6, step=1)\n","top_10_vocab = []\n","for i in range(0,5,1):\n","    top_10_vocab.append(model_list[0].get_topic_terms(i, topn = n_top_words))\n","feature_names = list(zip(*sorted(dictionary.items(), key=lambda x:x[0])))[1]\n","topics = [[feature_names[j[0]] for j in i] for i in top_10_vocab]\n","print(coherence_values)\n","print(topics)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[0.40446578073573114]\n","[['high', 'design', 'test', 'propose', 'space', 'power', 'develop', 'technology', 'control', 'nbsp', 'low', 'systems', 'materials', 'phase', 'performance', 'thermal', 'demonstrate', 'heat', 'cost', 'structure'], ['data', 'mar', 'nomenclature', 'mer', 'ops', 'set', 'spice', 'applicable', 'flow', 'rdr', 'contain', 'gazetteer', 'turbulence', 'camera', 'model', 'raw', 'ring', 'feature', 'planetary', 'surface'], ['data', 'power', 'set', 'mission', 'navigation', 'high', 'network', 'phase', 'model', 'base', 'academy', 'design', 'contain', 'project', 'sensor', 'file', 'space', 'gnss', 'image', 'spacecraft'], ['data', 'image', 'raw', 'radio', 'observations', 'archive', 'contain', 'orbiter', 'set', 'file', 'camera', 'rosetta', 'pds', 'tes', 'satellite', 'mission', 'cloud', 'rss', 'acquire', 'science'], ['data', 'set', 'band', 'product', 'global', 'image', 'resolution', 'products', 'file', 'surface', 'cover', 'cloud', 'contain', 'observations', 'time', 'degree', 'instrument', 'laser', 'measurements', 'land']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7Dt2_SbFYZEB","colab":{}},"source":["class ProdLDA(nn.Module):\n","\n","    def __init__(self, net_arch):\n","        super(ProdLDA, self).__init__()\n","        ac = net_arch\n","        self.net_arch = net_arch\n","        # encoder\n","        self.en1_fc     = nn.Linear(ac['num_input'], ac['en1_units'])             # 1995 -> 100\n","        self.en2_fc     = nn.Linear(ac['en1_units'], ac['en2_units'])             # 100  -> 100\n","        self.en2_drop   = nn.Dropout(0.2)\n","        self.mean_fc    = nn.Linear(ac['en2_units'], ac['num_topic'])             # 100  -> 50\n","        self.mean_bn    = nn.BatchNorm1d(ac['num_topic'])                      # bn for mean\n","        self.logvar_fc  = nn.Linear(ac['en2_units'], ac['num_topic'])             # 100  -> 50\n","        self.logvar_bn  = nn.BatchNorm1d(ac['num_topic'])                      # bn for logvar\n","        self.softplus = nn.Softplus()\n","        # z\n","        self.p_drop     = nn.Dropout(0.2)\n","        # decoder\n","        self.decoder    = nn.Linear(ac['num_topic'], ac['num_input'])             # 50   -> 1995\n","        self.decoder_bn = nn.BatchNorm1d(ac['num_input'])                      # bn for decoder\n","        # prior mean and variance as constant buffers\n","        prior_mean   = torch.zeros((1, ac['num_topic']), requires_grad=False)\n","        prior_var    = torch.zeros((1, ac['num_topic']), requires_grad=False).fill_(ac['variance'])\n","        prior_logvar = prior_var.log()\n","        self.register_buffer('prior_mean',    prior_mean)\n","        self.register_buffer('prior_var',     prior_var)\n","        self.register_buffer('prior_logvar',  prior_logvar)\n","\n","    def forward(self, model_input, compute_loss=False):\n","        # compute posterior\n","        en1 = self.softplus(self.en1_fc(model_input))                            # en1_fc   output\n","        en2 = self.softplus(self.en2_fc(en1))                              # encoder2 output\n","        en2 = self.en2_drop(en2)\n","        posterior_mean   = self.mean_bn(self.mean_fc(en2))          # posterior mean\n","        posterior_logvar = self.logvar_bn(self.logvar_fc(en2))          # posterior log variance\n","        posterior_var    = posterior_logvar.exp()\n","        # take sample\n","        eps = Variable(model_input.data.new().resize_as_(posterior_mean.data).normal_(), requires_grad=False) # noise\n","        z = posterior_mean + posterior_var.sqrt() * eps                 # reparameterization\n","        p = F.softmax(z, -1)                                                # mixture probability\n","        p = self.p_drop(p)\n","        # do reconstruction\n","        recon = F.softmax(self.decoder_bn(self.decoder(p)),-1)             # reconstructed distribution over vocabulary\n","\n","        if compute_loss:\n","            return recon, self.loss(model_input, recon, posterior_mean, posterior_logvar, posterior_var)\n","        else:\n","            return recon\n","\n","    def loss(self, model_input, recon, posterior_mean, posterior_logvar, posterior_var):\n","        # NL\n","        NL  = -(model_input * (recon+1e-10).log()).sum(-1)\n","        # KLD, see Section 3.3 of Akash Srivastava and Charles Sutton, 2017, \n","        # https://arxiv.org/pdf/1703.01488.pdf\n","        prior_mean   = self.prior_mean.expand_as(posterior_mean)\n","        prior_var    = self.prior_var.expand_as(posterior_mean)\n","        prior_logvar = self.prior_logvar.expand_as(posterior_mean)\n","        var_division    = posterior_var  / prior_var\n","        diff            = posterior_mean - prior_mean\n","        diff_term       = diff * diff / prior_var\n","        logvar_division = prior_logvar - posterior_logvar\n","        # put KLD together\n","        KLD = 0.5 * ( (var_division + diff_term + logvar_division).sum(-1) - self.net_arch['num_topic'] )\n","        # loss\n","        loss = (NL + KLD)\n","        # in traiming mode, return averaged loss. In testing mode, return individual loss\n","        return loss.sum()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"R78to1l8jPFk","colab":{}},"source":["def train(optimizer, model, data_loader, args, topics, texts, dictionary, num_epoch = 20):\n","    for epoch in range(num_epoch):\n","        loss_epoch = 0.0\n","        all_indices = 0\n","        best_coherence = 0\n","        model.train()                   # switch to training mode\n","        for batch in data_loader:\n","            optimizer.zero_grad()\n","            batch = batch.float().to(device)\n","            recon, loss = model(batch, compute_loss=True)\n","            # optimize\n","            loss.backward()             # backprop\n","            optimizer.step()            # update parameters\n","            # report\n","            loss_epoch += loss.item()    # add loss to loss_epoch\n","            all_indices += batch.shape[0]\n","        emb = model.decoder.weight.data.cpu().numpy().T\n","        feature_names = list(zip(*sorted(dictionary.items(), key=lambda x:x[0])))[1]\n","        topics = [[feature_names[j] for j in emb[i].argsort()[:-n_top_words - 1:-1]] for i in range(emb.shape[0])]\n","        coherencemodel = CoherenceModel(topics = topics, texts = texts, dictionary = dictionary, coherence = 'c_v', topn = n_top_words)\n","        curr_coherence = coherencemodel.get_coherence()\n","        if curr_coherence > best_coherence:\n","            best_topics = topics\n","            best_coherence = curr_coherence\n","        if epoch % 5 == 0:\n","            print('Epoch {}, loss={}'.format(epoch, loss_epoch / all_indices))\n","    return best_topics, best_coherence\n","\n","class TopicModel(Dataset):\n","\n","    def __init__(self, corpus = bow_corpus, len_dict = len(dictionary)):\n","        self.corpus = corpus\n","        self.len_dict = len_dict\n","\n","    def __len__(self):\n","        return len(self.corpus)\n","\n","    def __getitem__(self, idx):\n","        sample = np.zeros(self.len_dict)\n","        index, value = zip(*self.corpus[idx])\n","        sample[list(index)] = list(value)\n","        return sample       "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wNvDb0TKpGk_","colab":{}},"source":["args = {'num_input':len(dictionary),'en1_units': 100, 'en2_units': 100, 'num_topic': 6, 'batch_size': 128, 'init_mult': 1, 'variance': 0.995}\n","\n","dataset = TopicModel()\n","loader = torch.utils.data.DataLoader(dataset = dataset, \n","                                              batch_size = args['batch_size'],\n","                                              shuffle = True)\n","\n","def compute_coherence_prodLDA(dictionary, corpus, texts, limit, start=2, step=3):\n","    \"\"\"\n","    Compute c_v coherence for various number of topics\n","\n","    Parameters:\n","    ----------\n","    dictionary : Gensim dictionary\n","    corpus : Gensim corpus\n","    texts : List of input texts\n","    limit : Max num of topics\n","\n","    Returns:\n","    -------\n","    model_list : List of LDA topic models\n","    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n","    \"\"\"\n","    for num_topics in range(start, limit, step):\n","        args['num_topic'] = num_topics\n","        model = ProdLDA(args).to(device)\n","        optimizer = torch.optim.Adam([p for p in model.parameters() if p.requires_grad == True], 0.002, betas=(0.99, 0.999))\n","        best_topics, best_coherence = train(optimizer, model, loader, num_epoch = 50, args = args, topics = topics, texts = texts, dictionary = dictionary)\n","    return best_topics, best_coherence"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"g0mtwc3k9aBi","outputId":"60d32fed-cef5-458e-f4ea-e80ea36d6b58","executionInfo":{"status":"ok","timestamp":1575553243627,"user_tz":300,"elapsed":756680,"user":{"displayName":"Ravi Choudhary","photoUrl":"","userId":"10457362382565872664"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["best_topics, best_coherence = compute_coherence_prodLDA(dictionary=dictionary, corpus=corpus_tfidf, texts=df['tokenized_desc_title'], start=5, limit=6, step=1)\n","print(best_coherence)\n","print(best_topics)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Epoch 0, loss=983.389534345015\n","Epoch 5, loss=766.9374742618293\n","Epoch 10, loss=737.9754338812132\n","Epoch 15, loss=730.107826527303\n","Epoch 20, loss=726.3859255317506\n","Epoch 25, loss=724.1477855199626\n","Epoch 30, loss=722.8310340567475\n","Epoch 35, loss=720.7262412554564\n","Epoch 40, loss=719.6860502975333\n","Epoch 45, loss=718.6556707302949\n","0.7508825720105057\n","[['solitary', 'imf', 'tonian', 'strobes', 'xxxxxxwith', 'npudt', 'fuxxxxxxion', 'hiersesp', 'passband', 'derivable', 'reside', 'monte', 'phds', 'cptrb', 'strastospheric', 'moonlight', 'nonptofxphr', 'isas', 'nicl', 'basalts'], ['alkalis', 'physicochemical', 'solidify', 'catalysts', 'dry', 'lithograph', 'wastewater', 'hybridize', 'dioxide', 'mrad', 'oba', 'elucidate', 'boiloffcould', 'analyst', 'adiabatically', 'sspms', 'multiplexers', 'amseng', 'pem', 'dgu'], ['backscan', 'recorder', 'jump', 'xxxxxxi', 'ommydageo', 'alinp', 'bern', 'timenc', 'revolutions', 'xxxxxxgst', 'dtoz', 'shipboard', 'ae_si', 'xxxxxxghk', 'directorates', 'deceleration', 'revolution', 'qsx', 'ncayyyyddd', 'aloft'], ['similitude', 'througout', 'thestructures', 'lifemonitoring', 'decompostiition', 'mesostructure', 'tyvek', 'ttmr', 'panalysis', 'gflops', 'anthropometries', 'simulatesflow', 'fam', 'controlsuggest', 'flighttest', 'areidentified', 'easiest', 'redun', 'ddcus', 'countermasure'], ['gpwv', 'ucf', 'dice', 'cumulus', 'buss', 'projector', 'divergence', 'ribbon', 'hets', 'vemap', 'dfos', 'airflow', 'raptor', 'holography', 'spread', 'mfcs', 'finders', 'carpal', 'kilowatt', 'skim']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UxjScHia9ohX","colab":{}},"source":["Tensor = torch.FloatTensor\n","import torch.autograd as autograd\n","\n","class Generator(nn.Module):\n","    def __init__(self, opt):\n","        super(Generator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(opt['num_topics'], opt['enc_mid_layer']),\n","            nn.LeakyReLU(),\n","            # nn.LayerNorm(opt['enc_mid_layer']),\n","            nn.Linear(opt['enc_mid_layer'], opt['vocab_size']),\n","            nn.Softmax(-1)\n","        )\n","\n","    def forward(self, z):\n","        z = self.model(z)\n","        return z\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, opt):\n","        super(Discriminator, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(opt['vocab_size'], opt['dec_mid_layer']),\n","            nn.LeakyReLU(),\n","            nn.Linear(opt['dec_mid_layer'], 1)\n","        )\n","\n","    def forward(self, z):\n","        z = self.model(z)\n","        return z\n","\n","\n","def compute_gradient_penalty(D, real_samples, fake_samples):\n","    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n","    # Random weight term for interpolation between real and fake samples\n","    alpha = Tensor(np.random.random((real_samples.size(0), 1))).to(device)\n","    # Get random interpolation between real and fake samples\n","    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True).to(device)\n","    # print(interpolates.shape)\n","    d_interpolates = D(interpolates)\n","    # print(d_interpolates.shape)\n","    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False).to(device)\n","    # Get gradient w.r.t. interpolates\n","    gradients = autograd.grad(\n","        outputs=d_interpolates,\n","        inputs=interpolates,\n","        grad_outputs=fake,\n","        create_graph=True,\n","        retain_graph=True,\n","        only_inputs=True,\n","    )[0]\n","    gradients = gradients.view(gradients.size(0), -1)\n","    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n","    return gradient_penalty\n","\n","def init_weights(m):\n","    if type(m) == nn.Linear:\n","        nn.init.kaiming_normal_(m.weight)\n","\n","# ----------\n","#  Training\n","# ----------\n","def trainer(dataloader, opt, generator, discriminator, optimizer_G, optimizer_D, dictionary, corpus, texts):\n","    best_coherence = 0\n","    for epoch in range(opt['n_epochs']):\n","        for i, real_data in enumerate(dataloader):\n","\n","            # Configure input\n","            real_data = real_data.float().to(device)\n","\n","            # ---------------------\n","            #  Train Discriminator\n","            # ---------------------\n","\n","            optimizer_D.zero_grad()\n","\n","            dirichlet = torch.distributions.dirichlet.Dirichlet(torch.tensor([1/opt['num_topics'] for _ in range(opt['num_topics'])]))\n","            sample = dirichlet.sample()\n","\n","            # Sample noise as generator input\n","            z = Variable(sample.repeat(real_data.shape[0],1)).to(device)\n","\n","            # Generate a batch of images\n","            fake_data = generator(z)\n","\n","            # Real images\n","            real_validity = discriminator(real_data)\n","            # Fake fake_data\n","            fake_validity = discriminator(fake_data)\n","            # Gradient penalty\n","            gradient_penalty = compute_gradient_penalty(discriminator, real_data.data, fake_data.data)\n","            # Adversarial loss\n","            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + opt['lambda_gp'] * gradient_penalty\n","            wasserstein_d = -torch.mean(real_validity) + torch.mean(fake_validity)\n","            d_loss.backward()\n","            optimizer_D.step()\n","\n","            optimizer_G.zero_grad()\n","\n","            # Train the generator every n_critic steps\n","            if i % opt['n_critic'] == 0:\n","\n","                # -----------------\n","                #  Train Generator\n","                # -----------------\n","\n","                # Generate a batch of images\n","                fake_data = generator(z)\n","                # Loss measures generator's ability to fool the discriminator\n","                # Train on fake images\n","                fake_validity = discriminator(fake_data)\n","                g_loss = -torch.mean(fake_validity)\n","\n","                g_loss.backward()\n","                optimizer_G.step()\n","\n","        print(\n","            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] [Wasserstein Distance: %f]\"\n","            % (epoch, opt['n_epochs'], i, len(dataloader), d_loss.item(), g_loss.item(), wasserstein_d.item())\n","        )\n","        emb = [generator(Tensor([[0 if i != j else 1 for j in range(args['num_topics'])]]).to(device)).squeeze() for i in range(args['num_topics'])]\n","        feature_names = list(zip(*sorted(dictionary.items(), key=lambda x:x[0])))[1]\n","\n","        topics = [[feature_names[j] for j in emb[i].argsort(descending=True)[:n_top_words - 1]] for i in range(len(emb))]\n","        coherencemodel = CoherenceModel(topics = topics, texts = texts, dictionary = dictionary, coherence = 'c_v', topn = n_top_words)\n","        coherence_value= coherencemodel.get_coherence()\n","        if coherence_value > best_coherence:\n","            best_coherence = coherence_value\n","            best_topics = topics\n","    return best_topics, best_coherence"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QnF0R5MCcMKe","colab_type":"code","colab":{}},"source":["class TopicModel(Dataset):\n","\n","    def __init__(self, corpus = corpus_tfidf, len_dict = len(dictionary)):\n","        self.corpus = corpus\n","        self.len_dict = len_dict\n","\n","    def __len__(self):\n","        return len(self.corpus)\n","\n","    def __getitem__(self, idx):\n","        sample = np.zeros(self.len_dict)\n","        index, value = zip(*self.corpus[idx])\n","        sample[list(index)] = list(value)\n","        sample = sample/sample.sum()\n","        return sample"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LLCgb5R2Pp6V","colab":{}},"source":["args = {'num_topics': 5,'enc_mid_layer': 100, 'dec_mid_layer': 100, 'lambda_gp':10, 'vocab_size': len(dictionary), 'batch_size': 128, 'n_epochs': 50, 'n_critic': 5}\n","\n","dataset = TopicModel()\n","loader = torch.utils.data.DataLoader(dataset = dataset, \n","                                      batch_size = args['batch_size'],\n","                                      shuffle = True)\n","\n","def compute_coherence_ATM(dictionary, corpus, texts, limit, start=2, step=3):\n","    \"\"\"\n","    Compute c_v coherence for various number of topics\n","\n","    Parameters:\n","    ----------\n","    dictionary : Gensim dictionary\n","    corpus : Gensim corpus\n","    texts : List of input texts\n","    limit : Max num of topics\n","\n","    Returns:\n","    -------\n","    model_list : List of LDA topic models\n","    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n","    \"\"\"\n","    for num_topics in range(start, limit, step):\n","        args['num_topics'] = num_topics\n","        # Loss weight for gradient penalty\n","        lambda_gp = 10\n","\n","        # Initialize generator and discriminator\n","        generator = Generator(args).to(device)\n","        discriminator = Discriminator(args).to(device)\n","        generator.apply(init_weights)\n","        discriminator.apply(init_weights)\n","        # Optimizers\n","        optimizer_G = torch.optim.Adam(generator.parameters(), lr = 0.0001, betas=(0, 0.9))\n","        optimizer_D = torch.optim.Adam(discriminator.parameters(), lr = 0.0001, betas=(0, 0.9))\n","\n","        best_topics, best_coherence = trainer(loader, args, generator, discriminator, optimizer_G, optimizer_D, dictionary, corpus, texts)\n","    return best_topics, best_coherence\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MbBNiGHMbB47","colab_type":"code","outputId":"f20c44b9-c092-4e11-b097-c6e121ee1095","executionInfo":{"status":"ok","timestamp":1575554810235,"user_tz":300,"elapsed":2318870,"user":{"displayName":"Ravi Choudhary","photoUrl":"","userId":"10457362382565872664"}},"colab":{"base_uri":"https://localhost:8080/","height":894}},"source":["best_topics, best_coherence = compute_coherence_ATM(dictionary=dictionary, corpus=corpus_tfidf, texts=df['tokenized_desc_title'], start=5, limit=6, step=1)\n","print(best_topics, best_coherence)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["[Epoch 0/50] [Batch 189/190] [D loss: -0.002262] [G loss: -0.072527] [Wasserstein Distance: -0.012296]\n","[Epoch 1/50] [Batch 189/190] [D loss: 0.020193] [G loss: -0.069340] [Wasserstein Distance: -0.015688]\n","[Epoch 2/50] [Batch 189/190] [D loss: -0.004684] [G loss: -0.067394] [Wasserstein Distance: -0.017018]\n","[Epoch 3/50] [Batch 189/190] [D loss: -0.015532] [G loss: -0.065668] [Wasserstein Distance: -0.019149]\n","[Epoch 4/50] [Batch 189/190] [D loss: -0.009091] [G loss: -0.064334] [Wasserstein Distance: -0.018125]\n","[Epoch 5/50] [Batch 189/190] [D loss: -0.015378] [G loss: -0.063545] [Wasserstein Distance: -0.020598]\n","[Epoch 6/50] [Batch 189/190] [D loss: -0.019519] [G loss: -0.062506] [Wasserstein Distance: -0.020267]\n","[Epoch 7/50] [Batch 189/190] [D loss: -0.011450] [G loss: -0.062058] [Wasserstein Distance: -0.019475]\n","[Epoch 8/50] [Batch 189/190] [D loss: -0.015744] [G loss: -0.061586] [Wasserstein Distance: -0.019957]\n","[Epoch 9/50] [Batch 189/190] [D loss: -0.017228] [G loss: -0.060372] [Wasserstein Distance: -0.019975]\n","[Epoch 10/50] [Batch 189/190] [D loss: -0.022531] [G loss: -0.059613] [Wasserstein Distance: -0.022878]\n","[Epoch 11/50] [Batch 189/190] [D loss: -0.024484] [G loss: -0.058428] [Wasserstein Distance: -0.024976]\n","[Epoch 12/50] [Batch 189/190] [D loss: -0.014792] [G loss: -0.057488] [Wasserstein Distance: -0.023120]\n","[Epoch 13/50] [Batch 189/190] [D loss: -0.019345] [G loss: -0.056486] [Wasserstein Distance: -0.020210]\n","[Epoch 14/50] [Batch 189/190] [D loss: -0.021192] [G loss: -0.055866] [Wasserstein Distance: -0.021634]\n","[Epoch 15/50] [Batch 189/190] [D loss: -0.021832] [G loss: -0.055186] [Wasserstein Distance: -0.022167]\n","[Epoch 16/50] [Batch 189/190] [D loss: -0.022911] [G loss: -0.054259] [Wasserstein Distance: -0.023558]\n","[Epoch 17/50] [Batch 189/190] [D loss: -0.025230] [G loss: -0.053910] [Wasserstein Distance: -0.026309]\n","[Epoch 18/50] [Batch 189/190] [D loss: -0.027499] [G loss: -0.052821] [Wasserstein Distance: -0.027608]\n","[Epoch 19/50] [Batch 189/190] [D loss: -0.021693] [G loss: -0.052405] [Wasserstein Distance: -0.022568]\n","[Epoch 20/50] [Batch 189/190] [D loss: -0.024876] [G loss: -0.051595] [Wasserstein Distance: -0.025064]\n","[Epoch 21/50] [Batch 189/190] [D loss: -0.028887] [G loss: -0.051006] [Wasserstein Distance: -0.029322]\n","[Epoch 22/50] [Batch 189/190] [D loss: -0.025784] [G loss: -0.050334] [Wasserstein Distance: -0.026921]\n","[Epoch 23/50] [Batch 189/190] [D loss: -0.024361] [G loss: -0.049405] [Wasserstein Distance: -0.024835]\n","[Epoch 24/50] [Batch 189/190] [D loss: -0.022868] [G loss: -0.048538] [Wasserstein Distance: -0.024939]\n","[Epoch 25/50] [Batch 189/190] [D loss: -0.026393] [G loss: -0.047875] [Wasserstein Distance: -0.027376]\n","[Epoch 26/50] [Batch 189/190] [D loss: -0.024494] [G loss: -0.047855] [Wasserstein Distance: -0.025390]\n","[Epoch 27/50] [Batch 189/190] [D loss: -0.032823] [G loss: -0.047150] [Wasserstein Distance: -0.033961]\n","[Epoch 28/50] [Batch 189/190] [D loss: -0.024619] [G loss: -0.045423] [Wasserstein Distance: -0.031744]\n","[Epoch 29/50] [Batch 189/190] [D loss: -0.020713] [G loss: -0.044464] [Wasserstein Distance: -0.030269]\n","[Epoch 30/50] [Batch 189/190] [D loss: -0.026679] [G loss: -0.043821] [Wasserstein Distance: -0.027164]\n","[Epoch 31/50] [Batch 189/190] [D loss: -0.025694] [G loss: -0.043475] [Wasserstein Distance: -0.027411]\n","[Epoch 32/50] [Batch 189/190] [D loss: -0.027090] [G loss: -0.042360] [Wasserstein Distance: -0.028696]\n","[Epoch 33/50] [Batch 189/190] [D loss: -0.026955] [G loss: -0.041573] [Wasserstein Distance: -0.027374]\n","[Epoch 34/50] [Batch 189/190] [D loss: -0.039077] [G loss: -0.041060] [Wasserstein Distance: -0.039326]\n","[Epoch 35/50] [Batch 189/190] [D loss: -0.034597] [G loss: -0.040927] [Wasserstein Distance: -0.035825]\n","[Epoch 36/50] [Batch 189/190] [D loss: -0.036694] [G loss: -0.039746] [Wasserstein Distance: -0.037114]\n","[Epoch 37/50] [Batch 189/190] [D loss: -0.032136] [G loss: -0.039254] [Wasserstein Distance: -0.032898]\n","[Epoch 38/50] [Batch 189/190] [D loss: -0.039093] [G loss: -0.038467] [Wasserstein Distance: -0.040683]\n","[Epoch 39/50] [Batch 189/190] [D loss: -0.043543] [G loss: -0.037490] [Wasserstein Distance: -0.044238]\n","[Epoch 40/50] [Batch 189/190] [D loss: -0.038475] [G loss: -0.037703] [Wasserstein Distance: -0.039738]\n","[Epoch 41/50] [Batch 189/190] [D loss: -0.039408] [G loss: -0.036662] [Wasserstein Distance: -0.039712]\n","[Epoch 42/50] [Batch 189/190] [D loss: -0.041226] [G loss: -0.036447] [Wasserstein Distance: -0.041499]\n","[Epoch 43/50] [Batch 189/190] [D loss: -0.039357] [G loss: -0.035306] [Wasserstein Distance: -0.040365]\n","[Epoch 44/50] [Batch 189/190] [D loss: -0.042312] [G loss: -0.034923] [Wasserstein Distance: -0.043047]\n","[Epoch 45/50] [Batch 189/190] [D loss: -0.043454] [G loss: -0.034791] [Wasserstein Distance: -0.043977]\n","[Epoch 46/50] [Batch 189/190] [D loss: -0.039607] [G loss: -0.034401] [Wasserstein Distance: -0.040545]\n","[Epoch 47/50] [Batch 189/190] [D loss: -0.051304] [G loss: -0.033507] [Wasserstein Distance: -0.052179]\n","[Epoch 48/50] [Batch 189/190] [D loss: -0.040592] [G loss: -0.034519] [Wasserstein Distance: -0.040942]\n","[Epoch 49/50] [Batch 189/190] [D loss: -0.044018] [G loss: -0.033471] [Wasserstein Distance: -0.044895]\n","[['internally', 'rotational', 'plight', 'pyrogen', 'subcooling', 'airmoss', 'cb', 'ssl', 'horizons', 'thatthe', 'grabber', 'xxxxxxction', 'intumescents', 'verticies', 'condensational', 'resolvent', 'neatly', 'dac', 'competitiveness'], ['quatara', 'struc', 'nexgen', 'globe', 'safer', 'tenfold', 'cross', 'powerplants', 'whyme', 'lem', 'exospheres', 'xxxxxxxxsri', 'providea', 'accurately', 'gismo', 'yig', 'phasemeters', 'crco', 'halon'], ['inventoried', 'simu', 'fligh', 'utlized', 'assessment', 'phaseii', 'exoskeleton', 'capableof', 'subaperture', 'srls', 'irradiate', 'pah', 'benchmarked', 'anddeployable', 'tackle', 'subclusters', 'differencing', 'desorb', 'nxocn'], ['allowm', 'profilometers', 'backend', 'alinp', 'iunxlfo', 'fluidics', 'encode', 'unflagged', 'smallest', 'irve', 'vdw', 'suitport', 'hanpp', 'rocketfuel', 'ihl', 'brawn', 'ntiac', 'dbf', 'refractor'], ['overdrive', 'leo', 'pressurevessels', 'burke', 'amplitiude', 'demerits', 'xxxxxx_cpr_v', 'continuingly', 'nanofluids', 'odd', 'condensible', 'rpoe', 'andmergers', 'sweet', 'xxxxxxsail', 'eils', 'proceed', 'africa', 'incident']] 0.731749016535891\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x9etkH7TZ1LT","colab_type":"code","outputId":"643c741a-e572-48dc-f211-bc84668e61b8","executionInfo":{"status":"ok","timestamp":1575340304087,"user_tz":300,"elapsed":107945,"user":{"displayName":"Ravi Choudhary","photoUrl":"","userId":"10457362382565872664"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GDoHFOK1bg72","colab_type":"code","colab":{}},"source":["import pickle\n","coherence_values = pickle.load(open('/content/drive/My Drive/Topic_Modeling/GAN_Coherence.pkl','rb'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sYfn4xdZXl2Z","colab_type":"code","outputId":"e20a32b1-edd9-4729-b44e-51e8cde63112","executionInfo":{"status":"ok","timestamp":1575340316241,"user_tz":300,"elapsed":778,"user":{"displayName":"Ravi Choudhary","photoUrl":"","userId":"10457362382565872664"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(coherence_values)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["20"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"6K7bTYFVXnKW","colab_type":"code","outputId":"a0687e75-cefa-4243-d93a-67220b7d3ff6","executionInfo":{"status":"ok","timestamp":1575349064035,"user_tz":300,"elapsed":601,"user":{"displayName":"Ravi Choudhary","photoUrl":"","userId":"10457362382565872664"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":[""],"execution_count":0,"outputs":[{"output_type":"stream","text":["1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LXa3SCjXHv6Y","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}